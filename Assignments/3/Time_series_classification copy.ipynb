{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbbe45d1",
   "metadata": {},
   "source": [
    "## **Time series classification using tree-based methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2096b97",
   "metadata": {},
   "source": [
    "#### **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89581773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from collections import Counter\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de055a78",
   "metadata": {},
   "source": [
    "#### **Define distance functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7b03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x_slice, x_ref):\n",
    "\tx_ref_value = float(np.array(x_ref).flatten()[0])\n",
    "\treturn np.sqrt(np.sum((x_slice - x_ref_value) ** 2))\n",
    "\n",
    "def manhattan_distance(x_slice, x_ref):\n",
    "\tx_ref_value = float(np.array(x_ref).flatten()[0])\n",
    "\treturn np.sum(np.abs(x_slice - x_ref_value))\n",
    "\n",
    "def cosine_distance(x_slice, x_ref):\n",
    "\t\"\"\"\n",
    "\tDistanza coseno tra x_ref e x_slice trattata come vettore\n",
    "\tSe vuoi element-wise, considera x_ref broadcasted\n",
    "\t\"\"\"\n",
    "\tx_ref_value = float(np.array(x_ref).flatten()[0])\n",
    "\tx_slice_vec = np.array(x_slice)\n",
    "\t# coseno tra vettori: x_ref replicato\n",
    "\tdot = x_slice_vec * x_ref_value\n",
    "\tnorm = np.linalg.norm(x_slice_vec) * np.linalg.norm([x_ref_value])\n",
    "\treturn 1 - np.sum(dot) / (norm + 1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b24b1",
   "metadata": {},
   "source": [
    "#### **Node and PromptTree class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf05fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "\tdef __init__(self, x_barr, channel, bi, ei, df, t):\n",
    "\t\tself.x_barr = x_barr\n",
    "\t\tself.channel = channel\n",
    "\t\tself.begin_idx = bi\n",
    "\t\tself.end_idx = ei\n",
    "\t\tself.distance_f_node = df \n",
    "\t\tself.threshold = t\n",
    "\t\t\n",
    "\t\t# figli\n",
    "\t\tself.node_dx = None # Node()\n",
    "\t\tself.node_sx = None # Node()\n",
    "\n",
    "class LeafNode():\n",
    "\tdef __init__(self, class_f):\n",
    "\t\tself.classification_f = class_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e32569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTree():\n",
    "\tdef __init__(self, X, y, distance_function):\n",
    "\t\t# Require\n",
    "\t\tself.X = X\n",
    "\t\tself.y = y\n",
    "\t\tself.current_path = None  # lista di dizionari\n",
    "\t\tself.distance_functions = self.get_distance_functions()\n",
    "\t\tself.dist_func_sel = distance_function\n",
    "\n",
    "\t\tvalues, counts = np.unique(y, return_counts=True)\n",
    "\t\tself.most_common_class = values[np.argmax(counts)]  # use when leaf is not able to assign a class\n",
    "\n",
    "\t\tself.root = self.prompt_tree_fit_algo(self.X, self.y, self.current_path)\n",
    "\t\t# self.v = [] # vertices\n",
    "\t\t# self.e = [] # edges\n",
    "\t\t# self.s = [] # rst\n",
    "\t\t# self.l = [] # labels\n",
    "\n",
    "\tdef prompt_tree_fit_algo(self, X, y, path):\n",
    "\t\tif path is None:\n",
    "\t\t\tpath = []\n",
    "\t\t\tB = [0]\n",
    "\t\t\tE = [0]\n",
    "\t\telse:\n",
    "\t\t\tB, E = [], []\n",
    "\t\t\tfor node in path:\n",
    "\t\t\t\tB.append(node.begin_idx)\n",
    "\t\t\t\tE.append(node.end_idx)\n",
    "\n",
    "\t\t# Condizione di stop\n",
    "\t\tif self.stopping_f(path, X, y):\n",
    "\t\t\tif len(X) == 0:  # quando non ha una previsione\n",
    "\t\t\t\treturn LeafNode(None)\n",
    "\n",
    "\t\t\tleaf_class = self.classification_f(X, y)\n",
    "\t\t\treturn LeafNode(leaf_class)\n",
    "\n",
    "\t\t# Candidate intervals\n",
    "\t\tcandidate_intervals = self.promptness_f(X, y, B, np.max(E) + 1)\n",
    "\n",
    "\t\t# Candidate tests\n",
    "\t\tcandidate_tests = self.sampling_f(X, y, candidate_intervals)\n",
    "\n",
    "\t\t# Optimal candidate\n",
    "\t\toptimal_candidate, df_true, df_false = self.optimization_f(X, y, candidate_tests)\n",
    "\n",
    "\t\t# Recupero valori del nodo\n",
    "\t\tx_barr, c, begin_idx, end_idx, dist_f, threshold = (\n",
    "\t\t\toptimal_candidate[k] for k in ['x_barr', 'channel', 'b', 'e', 'dist_fun', 'threshold']\n",
    "\t\t)\n",
    "\n",
    "\t\t# Creazione nodo\n",
    "\t\tnode = Node(x_barr, c, begin_idx, end_idx, dist_f, threshold)\n",
    "\n",
    "\t\t# Preparazione dati per i rami sinistro e destro\n",
    "\t\tX_true = pd.DataFrame([x for x, _ in df_true], columns=X.columns)\n",
    "\t\ty_true = np.array([label for _, label in df_true])\n",
    "\t\tX_false = pd.DataFrame([x for x, _ in df_false], columns=X.columns)\n",
    "\t\ty_false = np.array([label for _, label in df_false])\n",
    "\n",
    "\t\t# Ricorsione ramo sinistro\n",
    "\t\tpath_true = path.copy()\n",
    "\t\tpath_true.append(node)\n",
    "\t\tnode.node_sx = self.prompt_tree_fit_algo(X_true, y_true, path_true)\n",
    "\n",
    "\t\t# Ricorsione ramo destro\n",
    "\t\tpath_false = path.copy()\n",
    "\t\tpath_false.append(node)\n",
    "\t\tnode.node_dx = self.prompt_tree_fit_algo(X_false, y_false, path_false)\n",
    "\n",
    "\t\treturn node\n",
    "\n",
    "\tdef promptness_f(self, X, y, B, max_e):\n",
    "\t\t''' Propone un set di coppie: canale, intervallo'''\n",
    "\t\ttotal_pairs = random.randint(1, len(B)//2 + 1)\n",
    "\t\tpairs = []\n",
    "\n",
    "\t\tchannels = list(X.columns)\n",
    "\t\tk = random.randint(1, len(channels))\n",
    "\t\tselected_channels = random.sample(channels, k)\n",
    "\n",
    "\t\tfor channel in selected_channels:\n",
    "\t\t\tfor i in range(total_pairs):\n",
    "\t\t\t\tb = random.randrange(B[0], max_e)\n",
    "\t\t\t\tpair = {'channel': channel, \"interval\": (b, max_e)}\n",
    "\t\t\t\tpairs.append(pair)\n",
    "\t\treturn pairs\n",
    "\n",
    "\tdef sampling_f(self, X, y, candidate_intervals):\n",
    "\t\tcandidate_tests = []\n",
    "\t\tfor candidate in candidate_intervals:\n",
    "\t\t\tchannel = candidate['channel']\n",
    "\t\t\tb = candidate['interval'][0]\n",
    "\t\t\te = candidate['interval'][1]\n",
    "\t\t\tdist_fun = self.dist_func_sel\n",
    "\t\t\tthreshold = random.uniform(0,2)\n",
    "\n",
    "\t\t\tref_idx = np.random.randint(0, X.shape[0])\n",
    "\t\t\tcols = [col for col in X.columns if col.startswith(channel)]\n",
    "\t\t\tx_barr = X.iloc[ref_idx][cols].apply(lambda s: s[b:e]).values\n",
    "\n",
    "\t\t\tcandidate_test = {\n",
    "\t\t\t\t'x_barr': x_barr,\n",
    "\t\t\t\t'channel': channel,\n",
    "\t\t\t\t'b': b,\n",
    "\t\t\t\t'e': e,\n",
    "\t\t\t\t'dist_fun': dist_fun,\n",
    "\t\t\t\t'threshold': threshold\n",
    "\t\t\t}\n",
    "\t\t\tcandidate_tests.append(candidate_test)\n",
    "\t\treturn candidate_tests\n",
    "\n",
    "\tdef optimization_f(self, X, y, candidate_tests):\n",
    "\t\tentropy_test = 1000\n",
    "\t\tdf_true = None\n",
    "\t\tdf_false = None\n",
    "\n",
    "\t\tfor candidate in candidate_tests:\n",
    "\t\t\tcandidate_tests_true = []\n",
    "\t\t\tcandidate_tests_false = []\n",
    "\n",
    "\t\t\tfor i, (idx, row) in enumerate(X.iterrows()):\n",
    "\t\t\t\tcol = candidate['channel']\n",
    "\t\t\t\tslice_x = row[col].iloc[candidate['b']:candidate['e']].values\n",
    "\t\t\t\tdist = np.mean(candidate['dist_fun'](slice_x, candidate['x_barr']))\n",
    "\n",
    "\t\t\t\tif dist <= candidate['threshold']:\n",
    "\t\t\t\t\tcandidate_tests_true.append((row, y[i]))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcandidate_tests_false.append((row, y[i]))\n",
    "\n",
    "\t\t\ty_true = [label for _, label in candidate_tests_true]\n",
    "\t\t\ty_false = [label for _, label in candidate_tests_false]\n",
    "\n",
    "\t\t\tcurrent_entropy = self.calculate_entropy(y_true, y_false)\n",
    "\t\t\tif current_entropy < entropy_test:\n",
    "\t\t\t\tbest_test_candidate = candidate\n",
    "\t\t\t\tentropy_test = current_entropy\n",
    "\t\t\t\tdf_true = candidate_tests_true\n",
    "\t\t\t\tdf_false = candidate_tests_false\n",
    "\n",
    "\t\treturn best_test_candidate, df_true, df_false\n",
    "\n",
    "\tdef classification_f(self, X, y):\n",
    "\t\tclasses, counts = np.unique(y, return_counts=True)\n",
    "\t\tdistr = {cls: cnt/len(y) for cls, cnt in zip(classes, counts)}\n",
    "\t\treturn distr\n",
    "\n",
    "\tdef stopping_f(self, path, X, y):\n",
    "\t\t\"\"\"\n",
    "\t\tStop quando l'entropia è zero (cioè tutti i campioni appartengono alla stessa classe).\n",
    "\t\t\"\"\"\n",
    "\t\tif len(y) == 0:\n",
    "\t\t\treturn True\n",
    "\n",
    "\t\t_, counts = np.unique(y, return_counts=True)\n",
    "\t\treturn len(counts) == 1\n",
    "\n",
    "\t\t\n",
    "\tdef entropy(self, labels):\n",
    "\t\tfrom collections import Counter\n",
    "\t\timport math\n",
    "\t\tn = len(labels)\n",
    "\t\tif n == 0:\n",
    "\t\t\treturn 0.0\n",
    "\t\tcounts = Counter(labels)\n",
    "\t\tprobs = [c/n for c in counts.values()]\n",
    "\t\treturn -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\t\n",
    "\tdef calculate_entropy(self, y_true, y_false):\n",
    "\t\tn_true = len(y_true)\n",
    "\t\tn_false = len(y_false)\n",
    "\t\ttotal = n_true + n_false\n",
    "\t\tH_true = self.entropy(y_true)\n",
    "\t\tH_false = self.entropy(y_false)\n",
    "\t\tH_total = (n_true/total) * H_true + (n_false/total) * H_false\n",
    "\t\treturn H_total\n",
    "\t\n",
    "\tdef get_distance_functions(self):\n",
    "\t\treturn [euclidean_distance, manhattan_distance, cosine_distance]\n",
    "\t\n",
    "\t# Post-pruning\n",
    "\tdef post_prune(self, validation_X, validation_y):\n",
    "\t\t\"\"\"Perform post-pruning on the tree using a validation set\"\"\"\n",
    "\t\tdef prune_node(node, X_val, y_val):\n",
    "\t\t\tif isinstance(node, LeafNode):\n",
    "\t\t\t\treturn node\n",
    "\n",
    "\t\t\t# Se non ci sono dati di validazione, crea foglia con la classe più comune\n",
    "\t\t\tif len(X_val) == 0:\n",
    "\t\t\t\treturn LeafNode({self.most_common_class: 1.0})\n",
    "\n",
    "\t\t\t# Split validation set by node threshold\n",
    "\t\t\tX_left_rows, y_left_rows = [], []\n",
    "\t\t\tX_right_rows, y_right_rows = [], []\n",
    "\n",
    "\t\t\tfor i in range(len(X_val)):\n",
    "\t\t\t\trow = X_val.iloc[i]\n",
    "\t\t\t\tslice_x = row[node.channel].iloc[node.begin_idx:node.end_idx].values\n",
    "\t\t\t\tdist = np.mean(node.distance_f_node(slice_x, node.x_barr))\n",
    "\t\t\t\tif dist <= node.threshold:\n",
    "\t\t\t\t\tX_left_rows.append(row)\n",
    "\t\t\t\t\ty_left_rows.append(y_val[i])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tX_right_rows.append(row)\n",
    "\t\t\t\t\ty_right_rows.append(y_val[i])\n",
    "\n",
    "\t\t\t# Recursively prune children\n",
    "\t\t\tif node.node_sx is not None:\n",
    "\t\t\t\tnode.node_sx = prune_node(node.node_sx,\n",
    "\t\t\t\t\t\t\t\t\t\t  pd.DataFrame(X_left_rows, columns=X_val.columns),\n",
    "\t\t\t\t\t\t\t\t\t\t  y_left_rows)\n",
    "\t\t\tif node.node_dx is not None:\n",
    "\t\t\t\tnode.node_dx = prune_node(node.node_dx,\n",
    "\t\t\t\t\t\t\t\t\t\t  pd.DataFrame(X_right_rows, columns=X_val.columns),\n",
    "\t\t\t\t\t\t\t\t\t\t  y_right_rows)\n",
    "\n",
    "\t\t\t# Evaluate replacing node with a leaf\n",
    "\t\t\tleaf_class = self.classification_f(X_val, y_val)\n",
    "\t\t\tleaf_node = LeafNode(leaf_class)\n",
    "\n",
    "\t\t\tcorrect_leaf = sum([1 for i in range(len(X_val))\n",
    "\t\t\t\t\t\t\t\tif y_val[i] == self.predict_sample_leaf(leaf_node, X_val.iloc[i])])\n",
    "\t\t\tcorrect_subtree = sum([1 for i in range(len(X_val))\n",
    "\t\t\t\t\t\t\t\t   if y_val[i] == self.predict_sample(node, X_val.iloc[i])])\n",
    "\n",
    "\t\t\t# Replace node with leaf if accuracy does not decrease\n",
    "\t\t\tmin_samples_prune = 5\n",
    "\t\t\timprovement_threshold = 0.01\n",
    "\t\t\tif len(X_val) >= min_samples_prune and (correct_leaf - correct_subtree) >= improvement_threshold:\n",
    "\t\t\t\treturn leaf_node\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn node\n",
    "\n",
    "\t\tself.root = prune_node(self.root, validation_X, validation_y)\n",
    "\n",
    "\tdef predict_sample_leaf(self, leaf_node, x):\n",
    "\t\t\"\"\"Return predicted class from leaf (most probable)\"\"\"\n",
    "\t\tif leaf_node.classification_f is None:\n",
    "\t\t\treturn None\n",
    "\t\treturn max(leaf_node.classification_f, key=leaf_node.classification_f.get)\n",
    "\n",
    "\tdef predict_sample(self, node, x):\n",
    "\t\t\"\"\"Predict label for a single sample\"\"\"\n",
    "\t\twhile not isinstance(node, LeafNode):\n",
    "\t\t\tslice_x = x[node.channel].iloc[node.begin_idx:node.end_idx].values\n",
    "\t\t\tdist = np.mean(node.distance_f_node(slice_x, node.x_barr))\n",
    "\t\t\tnode = node.node_sx if dist <= node.threshold else node.node_dx\n",
    "\t\treturn self.predict_sample_leaf(node, x)\n",
    "\t\n",
    "\t# Get path from tree\n",
    "\tdef get_leaf_node(self, tree, x):\n",
    "\t\t\"\"\"Get the leaf node where a sample ends up\"\"\"\n",
    "\t\tif tree.root is None:\n",
    "\t\t\treturn None\n",
    "\n",
    "\t\tnode = tree.root\n",
    "\t\twhile not isinstance(node, LeafNode):\n",
    "\t\t\t# print(len(x))\n",
    "\t\t\tslice_x = x[node.begin_idx:node.end_idx]\n",
    "\t\t\t# slice_x = x[node.channel][node.begin_idx:node.end_idx]\n",
    "\t\t\tdist = np.mean(node.distance_f_node(slice_x, node.x_barr))\n",
    "\t\t\tif dist <= node.threshold:\n",
    "\t\t\t\tnode = node.node_sx  # ramo True\n",
    "\t\t\telse:\n",
    "\t\t\t\tnode = node.node_dx  # ramo False\n",
    "\n",
    "\t\t\tif node is None:\n",
    "\t\t\t\treturn None\n",
    "\n",
    "\t\treturn node\n",
    "\t\n",
    "\tdef get_path_depth(self, tree, x):\n",
    "\t\t\"\"\"Get the path depth for a single sample\"\"\"\n",
    "\t\tif self.root is None:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\t\tnode = self.root\n",
    "\t\tdepth = 0\n",
    "\t\twhile not isinstance(node, LeafNode):\n",
    "\t\t\t# print(len(x))\n",
    "\t\t\tslice_x = x[node.begin_idx:node.end_idx]\n",
    "\t\t\t# slice_x = x[node.channel][node.begin_idx:node.end_idx]\n",
    "\t\t\tdist = np.mean(node.distance_f_node(slice_x, node.x_barr))\n",
    "\t\t\tif dist <= node.threshold:\n",
    "\t\t\t\tnode = node.node_sx  # ramo True\n",
    "\t\t\telse:\n",
    "\t\t\t\tnode = node.node_dx  # ramo False\n",
    "\t\t\tif node is None:\n",
    "\t\t\t\treturn None\n",
    "\t\t\tdepth += 1\n",
    "\t\treturn depth\n",
    "\t\n",
    "\tdef get_shared_path_depth(self, tree, x1 , x2):\n",
    "\t\t\"\"\"Get the depth of shared path between two samples\"\"\"\n",
    "\t\tif tree.root is None:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\t\tnode = tree.root\n",
    "\t\tdepth = 0\n",
    "\t\twhile not isinstance(node, LeafNode):\n",
    "\t\t\t\tslice_x1 = x1[node.begin_idx:node.end_idx]\n",
    "\t\t\t\tdist_x1 = np.mean(node.distance_f_node(slice_x1, node.x_barr))\n",
    "\t\t\t\t\n",
    "\t\t\t\tslice_x2 = x2[node.begin_idx:node.end_idx]\n",
    "\t\t\t\tdist_x2 = np.mean(node.distance_f_node(slice_x2, node.x_barr))\n",
    "\t\t\t\t\n",
    "\t\t\t\tif dist_x1 <= node.threshold:\n",
    "\t\t\t\t\tresult1 = True\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tresult1 = False\n",
    "\t\t\t\t\n",
    "\t\t\t\tif dist_x2 <= node.threshold:\n",
    "\t\t\t\t\tresult2 = True\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tresult2 = False\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tif result1 != result2:\n",
    "\t\t\t\t\tbreak  # Paths diverge\n",
    "\n",
    "\t\t\t\tdepth += 1\n",
    "\t\t\t\tnode = node.node_sx if result1 else node.node_dx\n",
    "\n",
    "\t\t\t\tif node is None:\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\treturn depth, node\n",
    "\n",
    "\tdef get_max_path_depth(self, tree, x1, x2):\n",
    "\t\t\"\"\"Get the maximum path depth for two samples\"\"\"\n",
    "\t\tdepth1 = self.get_path_depth(tree, x1)\n",
    "\t\tdepth2 = self.get_path_depth(tree, x2)\n",
    "\t\treturn max(depth1, depth2)\n",
    "\t\n",
    "\tdef go_to_leaf_from_shared_path(self, node, x):\n",
    "\t\ttest_list_x = [] # per ogni test tengo del nodo e il risultato\n",
    "\t\twhile not isinstance(node, LeafNode):\n",
    "\t\t\tslice_x = x[node.begin_idx:node.end_idx]\n",
    "\t\t\tdist_x = np.mean(node.distance_f_node(slice_x, node.x_barr))\n",
    "\t\t\t\n",
    "\t\t\tif dist_x <= node.threshold:\n",
    "\t\t\t\tresult = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tresult = False\n",
    "\t\t\ttest_list_x.append((node, result))\n",
    "\t\t\tnode = node.node_sx if result else node.node_dx\n",
    "\t\t\tif node is None:\n",
    "\t\t\t\tbreak\n",
    "\t\treturn test_list_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fc33c",
   "metadata": {},
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d31e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "\tdef __init__(self, n_trees, X_train, y_train, X_val, y_val, X_test, y_test, distance_function):\n",
    "\t\tself.number_of_trees = n_trees\n",
    "\t\tself.X_train = X_train\n",
    "\t\tself.y_train = y_train\n",
    "\t\tself.X_val = X_val\n",
    "\t\tself.y_val = y_val\n",
    "\t\tself.X_test = X_test\n",
    "\t\tself.y_test = y_test\n",
    "\t\t\n",
    "\t\t# solo per conformal classifier\n",
    "\t\tself.X_cal = X_val\n",
    "\t\tself.y_cal = y_val\n",
    "\t\t\n",
    "\t\tself.dist_fun_for_tree = distance_function\n",
    "\t\tself.trees = self.create_trees()\n",
    "\n",
    "\t\t# dizionari per salvare pesi e track record\n",
    "\t\tself.weights_random_forest = None\n",
    "\t\tself.track_records_random_forest = None\n",
    "\n",
    "\tdef create_trees(self):\n",
    "\t\ttrees = {}\n",
    "\t\tfor i in range(self.number_of_trees):\n",
    "\t\t\ttree = PromptTree(self.X_train, self.y_train, self.dist_fun_for_tree)\n",
    "\t\t\ttrees[i] = tree\n",
    "\t\treturn trees \n",
    "\t\n",
    "\tdef predict_only(self, X):\n",
    "\t\t\"\"\"\n",
    "\t\tRestituisce:\n",
    "\t\t  - y_probs_final: lista di dizionari con la distribuzione finale combinata di tutti gli alberi\n",
    "\t\t  - y_probs_per_tree: lista di liste di dizionari, ciascuna sotto-lista contiene le distribuzioni di probabilità\n",
    "\t\t\tdi ciascun albero per un campione\n",
    "\t\t\"\"\"\n",
    "\t\ty_probs_final = []\n",
    "\t\ty_probs_per_tree = []  # lista di liste\n",
    "\n",
    "\t\tfor _, x in X.iterrows():\n",
    "\t\t\tclass_scores = {}\n",
    "\t\t\tprobs_per_tree = []\n",
    "\n",
    "\t\t\tfor tree in self.trees.values():\n",
    "\t\t\t\tpred = predict(tree, x, default_class=0)\n",
    "\t\t\t\t# Ogni albero restituisce solo 1 probabilità 1 per la predetta e 0 per le altre\n",
    "\t\t\t\tprobs_tree = {pred: 1.0}  # dizionario con predizione dell'albero\n",
    "\t\t\t\tprobs_per_tree.append(probs_tree)\n",
    "\n",
    "\t\t\t\t# accumula punteggio per combinazione finale\n",
    "\t\t\t\tclass_scores[pred] = class_scores.get(pred, 0) + 1\n",
    "\n",
    "\t\t\t# Normalizzazione in probabilità finale\n",
    "\t\t\ttotal_score = sum(class_scores.values())\n",
    "\t\t\tprobs_final = {cls: score / total_score for cls, score in class_scores.items()}\n",
    "\n",
    "\t\t\ty_probs_final.append(probs_final)\n",
    "\t\t\ty_probs_per_tree.append(probs_per_tree)\n",
    "\n",
    "\t\treturn y_probs_final, y_probs_per_tree\n",
    "\n",
    "\n",
    "\tdef majority_voting(self, X, return_probs=False):\n",
    "\t\ty_final_pred = []\n",
    "\t\ty_probs = [] \n",
    "\t\tfor _, x in X.iterrows():\n",
    "\t\t\tclass_votes = {}\n",
    "\t\t\tfor tree in self.trees.values():\n",
    "\t\t\t\tpred = predict(tree, x, default_class=0)\n",
    "\t\t\t\tclass_votes[pred] = class_votes.get(pred, 0) + 1\n",
    "\t\t\t\n",
    "\t\t\t# predizione finale\n",
    "\t\t\tfinal_cls = max(class_votes.items(), key=lambda kv: kv[1])[0]\n",
    "\t\t\ty_final_pred.append(final_cls)\n",
    "\t\t\t\n",
    "\t\t\tif return_probs:\n",
    "\t\t\t\ttotal_votes = sum(class_votes.values())\n",
    "\t\t\t\tprobs = {cls: v/total_votes for cls, v in class_votes.items()}\n",
    "\t\t\t\ty_probs.append(probs)\n",
    "\t\t\n",
    "\t\taccuracy_test = sum(p == t for p, t in zip(y_final_pred, self.y_test)) / len(self.y_test)\n",
    "\t\tif return_probs:\n",
    "\t\t\treturn round(accuracy_test, 3), y_final_pred, y_probs\n",
    "\t\telse:\n",
    "\t\t\treturn round(accuracy_test, 3)\n",
    "\n",
    "\tdef weighted_voting(self, X_val, y_val, X_test, y_test, return_probs=False):\n",
    "\t\t# Calcolo le accuracy in validation per ogni tree\n",
    "\t\taccuracies_val = {}\n",
    "\t\tfor i, tree in self.trees.items():\n",
    "\t\t\ty_pred = [predict(tree, x, default_class=0) for _, x in X_val.iterrows()]\n",
    "\t\t\tacc = sum(y_pred_i == y_true_i for y_pred_i, y_true_i in zip(y_pred, y_val)) / len(y_val)\n",
    "\t\t\taccuracies_val[i] = acc\n",
    "\n",
    "\t\t# Normalizzo\n",
    "\t\ttotal = sum(accuracies_val.values())\n",
    "\t\tif total == 0:\n",
    "\t\t\tweights = {i: 1/len(self.trees) for i in self.trees}\n",
    "\t\telse:\n",
    "\t\t\tweights = {i: acc/total for i, acc in accuracies_val.items()}\n",
    "\n",
    "\t\t# Salvo i pesi nella variabile di istanza\n",
    "\t\tself.weights_random_forest = weights\n",
    "\n",
    "\t\t# Final prediction pesata\n",
    "\t\ty_final_pred = []\n",
    "\t\ty_probs = []\n",
    "\t\tfor _, x in X_test.iterrows():\n",
    "\t\t\tclass_scores = {}\n",
    "\t\t\tfor i, tree in self.trees.items():\n",
    "\t\t\t\tpred = predict(tree, x, default_class=0)\n",
    "\t\t\t\tclass_scores[pred] = class_scores.get(pred, 0) + weights[i]\n",
    "\n",
    "\t\t\tfinal_cls = max(class_scores.items(), key=lambda kv: kv[1])[0]\n",
    "\t\t\ty_final_pred.append(final_cls)\n",
    "\n",
    "\t\t\tif return_probs:\n",
    "\t\t\t\ttotal_score = sum(class_scores.values())\n",
    "\t\t\t\tprobs = {cls: s/total_score for cls, s in class_scores.items()}\n",
    "\t\t\t\ty_probs.append(probs)\n",
    "\n",
    "\t\taccuracy_test = sum(p == t for p, t in zip(y_final_pred, y_test)) / len(y_test)\n",
    "\t\tif return_probs:\n",
    "\t\t\treturn round(accuracy_test, 3), y_final_pred, y_probs\n",
    "\t\telse:\n",
    "\t\t\treturn round(accuracy_test, 3)\n",
    "\n",
    "\tdef track_record_voting(self, X_val, y_val, X_test, y_test, return_probs=True):\n",
    "\t\t# Ogni tree accumula un record di performance su un validation set\n",
    "\t\taccuracies_val_for_tree = {}\n",
    "\t\tfor i, tree in self.trees.items():\n",
    "\t\t\tclass_correct = {}\n",
    "\t\t\tclass_total = {}\n",
    "\n",
    "\t\t\tfor idx, (_, x) in enumerate(X_val.iterrows()):\n",
    "\t\t\t\ty_pred_i = predict(tree, x, default_class=0)\n",
    "\t\t\t\ty_true_i = y_val[idx]\n",
    "\n",
    "\t\t\t\t# incremento i conteggi per la classe vera\n",
    "\t\t\t\tclass_total[y_true_i] = class_total.get(y_true_i, 0) + 1\n",
    "\t\t\t\tif y_pred_i == y_true_i:\n",
    "\t\t\t\t\tclass_correct[y_true_i] = class_correct.get(y_true_i, 0) + 1\n",
    "\n",
    "\t\t\t# salvo accuracies per classe\n",
    "\t\t\taccuracies_val_for_tree[i] = {\n",
    "\t\t\t\tcls: class_correct.get(cls, 0) / total for cls, total in class_total.items()\n",
    "\t\t\t}\n",
    "\n",
    "\t\t# Salvo il track record nella variabile di istanza\n",
    "\t\tself.track_records_random_forest = accuracies_val_for_tree\n",
    "\n",
    "\t\t# Final prediction\n",
    "\t\ty_final_pred = []\n",
    "\t\ty_probs = []\n",
    "\t\tfor _, x in X_test.iterrows():\n",
    "\t\t\tclass_scores = {}\n",
    "\t\t\tfor i, tree in self.trees.items():\n",
    "\t\t\t\tpred = predict(tree, x, default_class=0)\n",
    "\t\t\t\tweight = accuracies_val_for_tree[i].get(pred, 0)\n",
    "\t\t\t\tclass_scores[pred] = class_scores.get(pred, 0) + weight\n",
    "\n",
    "\t\t\tfinal_cls = max(class_scores.items(), key=lambda kv: kv[1])[0]\n",
    "\t\t\ty_final_pred.append(final_cls)\n",
    "\n",
    "\t\t\tif return_probs:\n",
    "\t\t\t\ttotal_score = sum(class_scores.values())\n",
    "\t\t\t\tif total_score == 0:\n",
    "\t\t\t\t\t# Distribuisci uniformemente le probabilità tra tutte le classi\n",
    "\t\t\t\t\tn_classes = len(class_scores)\n",
    "\t\t\t\t\tprobs = {cls: 1/n_classes for cls in class_scores}\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprobs = {cls: s/total_score for cls, s in class_scores.items()}\n",
    "\t\t\t\ty_probs.append(probs)\n",
    "\n",
    "\n",
    "\t\taccuracy_test = sum(p == t for p, t in zip(y_final_pred, y_test)) / len(y_test)\n",
    "\t\tif return_probs:\n",
    "\t\t\treturn round(accuracy_test, 3), y_final_pred, y_probs\n",
    "\t\telse:\n",
    "\t\t\treturn round(accuracy_test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a18e7f",
   "metadata": {},
   "source": [
    "#### **Conformal classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformalClassifier:\n",
    "\tdef __init__(self, classifier, voting=\"majority\", alpha=0.1):\n",
    "\t\t\"\"\"\n",
    "\t\tclassifier: istanza della tua RandomForest\n",
    "\t\tvoting: \"majority\", \"weighted\", \"track_record\"\n",
    "\t\t\"\"\"\n",
    "\t\tself.classifier = classifier\n",
    "\t\tself.voting = voting\n",
    "\t\tself.alpha = alpha\n",
    "\n",
    "\tdef get_pred_probs_calibration(self):\n",
    "\t\t\"\"\"\n",
    "\t\tRitorna le predizioni probabilistiche sul set di calibrazione\n",
    "\t\tsecondo il meccanismo di voting scelto.\n",
    "\t\t\"\"\"\n",
    "\t\tX_cal = self.classifier.X_cal \n",
    "\t\ty_cal = self.classifier.y_cal\n",
    "\t\ty_probs = []\n",
    "\t\t\n",
    "\t\ty_cal_preds_probs, y_cal_preds_probs_per_tree = self.classifier.predict_only(X_cal)\n",
    "\n",
    "\t\tif self.voting == \"majority\":\n",
    "\t\t\tprobs = y_cal_preds_probs\n",
    "\t\telif self.voting == \"weighted\":\n",
    "\t\t\t# best_pred_for_each_sample = []\n",
    "\t\t\tfinal_probs = []\n",
    "\t\t\tfor pred_prob_per_tree in y_cal_preds_probs_per_tree: # itero sulle predizioni per ogni sample\n",
    "\t\t\t\tcombined_scores = {}\n",
    "\t\t\t\tfor idx_of_tree, tree_probs in enumerate(pred_prob_per_tree):\n",
    "\t\t\t\t\t# print(str(tree_probs) + \" ---- \" + str(idx_of_tree))\n",
    "\t\t\t\t\tfor label, prob in tree_probs.items():\n",
    "\t\t\t\t\t# print(self.classifier.weights_random_forest)\n",
    "\t\t\t\t\t\ttree_weight = self.classifier.weights_random_forest[idx_of_tree] # peso dell'albero\n",
    "\t\t\t\t\t\tcombined_scores[label] = combined_scores.get(label, 0) + prob * tree_weight\n",
    "\t\t\t\ttotal_score = sum(combined_scores.values())\n",
    "\t\t\t\tif total_score > 0:\n",
    "\t\t\t\t\tnormalized_scores = {cls: score / total_score for cls, score in combined_scores.items()}\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnormalized_scores = {cls: 0 for cls in combined_scores}\n",
    "\n",
    "\t\t\t\t# tengo solo la predizione con prob massima\n",
    "\t\t\t\t# best_pred = max(normalized_scores.items(), key=lambda kv: kv[1])\n",
    "\t\t\t\t# best_pred = {best_pred[0]: best_pred[1]}\n",
    "\t\t\t\t# best_pred_for_each_sample.append(best_pred)\n",
    "\t\t\t\tfinal_probs.append(normalized_scores)\n",
    "\t\t\tprobs = final_probs\n",
    "\t\telif self.voting == \"track_record\":\n",
    "\t\t\t# best_pred_for_each_sample = []\n",
    "\t\t\tfinal_probs = []\n",
    "\t\t\tfor pred_prob_per_tree in y_cal_preds_probs_per_tree:  # itero sulle predizioni per ogni sample\n",
    "\t\t\t\tcombined_scores = {}\n",
    "\t\t\t\tfor idx_of_tree, tree_probs in enumerate(pred_prob_per_tree):  # per ogni albero\n",
    "\t\t\t\t\tfor label, prob in tree_probs.items():\n",
    "\t\t\t\t\t\t# prendo il peso dal track record dell'albero per quella classe\n",
    "\t\t\t\t\t\ttree_record = self.classifier.track_records_random_forest[idx_of_tree]\n",
    "\t\t\t\t\t\tclass_weight = tree_record.get(label, 0)  # prendo il peso relativo alla classe specifica, se quella classe non era mai stata vista → 0\n",
    "\t\t\t\t\t\tcombined_scores[label] = combined_scores.get(label, 0) + prob * class_weight\n",
    "\t\t\t\t\n",
    "\t\t\t\t# normalizzazione dei punteggi per ottenere distribuzione di probabilità\n",
    "\t\t\t\ttotal_score = sum(combined_scores.values())\n",
    "\t\t\t\tif total_score > 0:\n",
    "\t\t\t\t\tnormalized_scores = {cls: score / total_score for cls, score in combined_scores.items()}\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnormalized_scores = {cls: 0 for cls in combined_scores}\n",
    "\t\t\t\n",
    "\t\t\t\t# tengo solo la predizione con prob massima\n",
    "\t\t\t\t# best_pred = max(normalized_scores.items(), key=lambda kv: kv[1])\n",
    "\t\t\t\t# best_pred = {best_pred[0]: best_pred[1]}\n",
    "\t\t\t\t# best_pred_for_each_sample.append(best_pred)\n",
    "\t\t\t\tfinal_probs.append(normalized_scores)\n",
    "\t\t\tprobs = final_probs\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"Voting must be 'majority', 'weighted' or 'track_record'\")\n",
    "\t\t# print(probs)\n",
    "\t\t# print(len(probs))\n",
    "\t\treturn probs, y_cal\n",
    "\n",
    "\tdef conformal_predict(self):\n",
    "\t\t\"\"\"\n",
    "\t\tCalcola le predizioni conformi sul test set.\n",
    "\t\tRitorna una lista di liste: classi conformi per ogni sample.\n",
    "\t\t\"\"\"\n",
    "\t\t# Calcolo predizioni probabilistiche sul set di calibrazione\n",
    "\t\tpred_probs_cal, y_cal = self.get_pred_probs_calibration()\n",
    "\n",
    "\t\t# Calcolo i non-conformity scores\n",
    "\t\tnc_scores = []\n",
    "\t\tfor i, probs in enumerate(pred_probs_cal):\n",
    "\t\t\ttrue_class = y_cal[i] # questa è la vera label, la vera classe\n",
    "\t\t\t# print(true_class, i, probs)\n",
    "\t\t\tnc_score = 1 - probs.get(true_class, 0) # capisco quanto la mia predizione della classe vera si discosta dalla predizione della classe vera del dataset\n",
    "\t\t\tnc_scores.append(nc_score)\n",
    "\t\t# print(\"\\nNC scores\", nc_scores)\n",
    "\t\t# print(len(nc_scores))\n",
    "\n",
    "\t\t# Calcolo il threshold di conformità\n",
    "\t\tthreshold = np.quantile(nc_scores, 1 - self.alpha)\n",
    "\n",
    "\t\t# Predizione su X_test\n",
    "\t\ty_conformal = []\n",
    "\t\t# for _, x in X_test.iterrows():\n",
    "\t\t# Ottengo probabilità predette dal modello\n",
    "\t\tif self.voting == \"majority\":\n",
    "\t\t\t_, _, probs_test = self.classifier.majority_voting(self.classifier.X_test, return_probs=True)\n",
    "\t\telif self.voting == \"weighted\":\n",
    "\t\t\t_, _, probs_test = self.classifier.weighted_voting(self.classifier.X_val, self.classifier.y_val, self.classifier.X_test, self.classifier.y_test, return_probs=True)\n",
    "\t\telif self.voting == \"track_record\":\n",
    "\t\t\t_, _, probs_test = self.classifier.track_record_voting(self.classifier.X_val, self.classifier.y_val, self.classifier.X_test, self.classifier.y_test, return_probs=True)\n",
    "\n",
    "\t\t# Conformal set per questo sample\n",
    "\t\t# probs_test è lista di dict, un dict per sample contenente tutte le label e le relative probabilità\n",
    "\t\t# print(probs_test)\n",
    "\t\tfor i, sample_probs in enumerate(probs_test):  # iteriamo su ogni sample\n",
    "\t\t\tconformal_set = [\n",
    "\t\t\t\tcls\n",
    "\t\t\t\tfor cls, p in sample_probs.items() # prendo la classe/label e la probabilità relativa\n",
    "\t\t\t\tif (1 - p) <= threshold # memorizzo la classe nel conformal set solo se > threshold\n",
    "\t\t\t]\n",
    "\n",
    "\t\t\t# Evita duplicati (non necessario se sample_probs è già un dict)\n",
    "\t\t\ty_conformal.append(conformal_set)\n",
    "\n",
    "\t\treturn y_conformal\n",
    "\t\t\n",
    "\tdef calibrate(self, alpha=0.1):\n",
    "\t\ty_conformal = self.conformal_predict() # miscalibration=coverage desiderata−coverage osservata\n",
    "\t\t\n",
    "\t\t# Coverage desiderata\n",
    "\t\tconfidence = 1 - alpha\n",
    "\t\t\n",
    "\t\t# Efficiency = percentuale di sample con solo 1 classe conforme\n",
    "\t\tefficiency = sum(len(c) == 1 for c in y_conformal) / len(y_conformal)\n",
    "\t\t\n",
    "\t\t# Coverage osservata: quante volte la vera classe è nell'insieme conforme\n",
    "\t\tcoverage_observed = sum(\n",
    "\t\t\ttrue_label in conf_classes\n",
    "\t\t\tfor true_label, conf_classes in zip(self.classifier.y_cal, y_conformal)\n",
    "\t\t) / len(y_conformal)\n",
    "\t\t\n",
    "\t\t# Miscalibration\n",
    "\t\tmiscalibration = confidence - coverage_observed\n",
    "\t\t\n",
    "\t\treturn confidence, efficiency, miscalibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203cdbe",
   "metadata": {},
   "source": [
    "#### **Unsupervised learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebdbf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance functions\n",
    "def breiman_distance(forest, x1, x2):\n",
    "\t\"\"\"Breiman distance: fraction of trees where samples end up in different leaves\"\"\"\n",
    "\tif not forest.trees:\n",
    "\t\treturn 1.0\n",
    "\n",
    "\tdifferent_leaves = 0\n",
    "\tvalid_trees = 0\n",
    "\tfor idx_tree, tree in forest.trees.items(): # per ogni albero\n",
    "\t\t# Get leaf nodes for both samples, that are a list of numbers\n",
    "\t\t# print(\"X1:\", x1)\n",
    "\t\t# print(\"X2:\", x2)\n",
    "\t\tleaf1 = tree.get_leaf_node(tree, x1)\n",
    "\t\tleaf2 = tree.get_leaf_node(tree, x2)\n",
    "\n",
    "\t\tif leaf1 != leaf2:\n",
    "\t\t\tdifferent_leaves += 1\n",
    "\t\tvalid_trees += 1\n",
    "\treturn different_leaves / valid_trees if valid_trees > 0 else 1.0\n",
    "\n",
    "def zhu_distance(forest, x1, x2):\n",
    "\t\"\"\"Zhu distance: based on shared path length in trees\"\"\"\n",
    "\tif not forest.trees:\n",
    "\t\treturn 1.0\n",
    "\n",
    "\ttotal_shared_depth = 0\n",
    "\ttotal_max_depth = 0\n",
    "\n",
    "\tfor idx_tree, tree in forest.trees.items(): # per ogni albero\n",
    "\t\tshared_depth, _ = tree.get_shared_path_depth(tree, x1, x2)\n",
    "\t\tmax_depth = tree.get_max_path_depth(tree, x1, x2)\n",
    "\n",
    "\t\ttotal_shared_depth += shared_depth\n",
    "\t\ttotal_max_depth += max_depth\n",
    "\n",
    "\tif total_max_depth == 0:\n",
    "\t\treturn 1.0\n",
    "\n",
    "\treturn 1.0 - (total_shared_depth / total_max_depth)\n",
    "\n",
    "def ratio_rf_distance(forest, x1, x2):\n",
    "\t\"\"\"RatioRF distance: ratio-based distance measure\"\"\"\n",
    "\tif not forest.trees:\n",
    "\t\treturn 1.0\n",
    "\n",
    "\tshared_results_for_tree = {}\n",
    "\tfor idx_tree, tree in forest.trees.items(): # per ogni albero\n",
    "\t\tshared_depth, node = tree.get_shared_path_depth(tree, x1, x2) # prendo il path condiviso nell'albero\n",
    "\t\tmax_depth = tree.get_max_path_depth(tree, x1, x2) # to normalize value when shared_results_x1 or shared_results_x2 are zero \n",
    "\t\t\n",
    "\t\t# print('Shared depth:', shared_depth)\n",
    "\t\t# print('Max depth:', max_depth)\n",
    "\t\t# adesso sia per x1 che x2 colleziono i testi mentre scendono nell'albero dall'ultimo nodo condiviso\n",
    "\t\t\n",
    "\t\ttest_list_x1 = tree.go_to_leaf_from_shared_path(node, x1) # ritornano una tupla (node, result(True/False))\n",
    "\t\ttest_list_x2 = tree.go_to_leaf_from_shared_path(node, x2)\n",
    "\n",
    "\t\t# itero attraverso il dict, faccio il test sull'altro sample e confronto i risultati\n",
    "\t\tdiff_results_x1 = 0\n",
    "\t\tfor node_and_result in test_list_x1:\n",
    "\t\t\tnode = node_and_result[0]\n",
    "\t\t\tresult_x1 = node_and_result[1]\n",
    "\t\t\tslice_x2 = x2[node.begin_idx:node.end_idx]\n",
    "\t\t\tdist_x2 = np.mean(node.distance_f_node(slice_x2, node.x_barr))\n",
    "\t\t\t\n",
    "\t\t\tif dist_x2 <= node.threshold:\n",
    "\t\t\t\tresult_x2 = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tresult_x2 = False\n",
    "\t\t\t\n",
    "\t\t\t# confronti i risultati\n",
    "\t\t\tif result_x1 != result_x2: \n",
    "\t\t\t\tdiff_results_x1 += 1 # conto le discrepanze\n",
    "\t\t\n",
    "\t\tdiff_results_x2 = 0\n",
    "\t\tfor node_and_result in test_list_x2:\n",
    "\t\t\tnode = node_and_result[0]\n",
    "\t\t\tresult_x2 = node_and_result[1]\n",
    "\t\t\tslice_x1 = x1[node.begin_idx:node.end_idx]\n",
    "\t\t\tdist_x1 = np.mean(node.distance_f_node(slice_x1, node.x_barr))\n",
    "\t\t\t\n",
    "\t\t\tif dist_x1 <= node.threshold:\n",
    "\t\t\t\tresult_x1 = True\n",
    "\t\t\telse:\n",
    "\t\t\t\tresult_x1 = False\n",
    "\t\t\t\n",
    "\t\t\t# confronti i risultati\n",
    "\t\t\tif result_x1 != result_x2:\n",
    "\t\t\t\tdiff_results_x2 += 1\n",
    "\t\t\n",
    "\t\t############\n",
    "\t\t\n",
    "\t\t# based on tversky formula, alpha and beta are the 2 weights to give to the two shared results\n",
    "\t\talpha = 0.5 # give same importance\n",
    "\t\tbeta = 0.5 \n",
    "\t\t\n",
    "\t\tif diff_results_x1 == 0 and diff_results_x2 == 0:\n",
    "\t\t\tshared_results_for_tree[idx_tree] = shared_depth / max_depth # normalizzo lo shared path rispetto la distanza totale\n",
    "\t\telse:\n",
    "\t\t\tshared_results_for_tree[idx_tree] = shared_depth / (shared_depth + alpha * diff_results_x1 + beta * diff_results_x2)\n",
    "\t\n",
    "\tsimilarity = 0\n",
    "\t# print(\"Shared results for tree:\", shared_results_for_tree)\n",
    "\tfor _, shared_result in shared_results_for_tree.items():\n",
    "\t\tsimilarity += shared_result\n",
    "\treturn 1.0 - similarity / forest.number_of_trees\n",
    "\n",
    "# distance function for cluster similarity\n",
    "def euclidean_distance_clustering(x1, x2):\n",
    "\t\"\"\"Calcola la distanza Euclidea tra due sample flattenati§\"\"\"\n",
    "\tx1 = np.array(x1)\n",
    "\tx2 = np.array(x2)\n",
    "\n",
    "\treturn np.linalg.norm(x1 - x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47a553",
   "metadata": {},
   "source": [
    "##### **Isolation forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationForest:\n",
    "\n",
    "\tdef __init__(self, rf, n_clusters, distance_type):\n",
    "\t\tself.forest = rf\n",
    "\t\tself.distance_functions = self.get_distance_functions()\n",
    "\t\tself.n_clusters = n_clusters\n",
    "\t\tself.distance_type = distance_type\n",
    "\t\tself.euclidean_distance = euclidean_distance_clustering\n",
    "\n",
    "\tdef get_distance_functions(self):\n",
    "\t\treturn {\n",
    "\t\t\t'breiman': breiman_distance,\n",
    "\t\t\t'zhu': zhu_distance,\n",
    "\t\t\t'ratio_rf': ratio_rf_distance\n",
    "\t\t}\n",
    "\t\n",
    "\tdef perform_cluster(self, X):\n",
    "\t\t\"\"\"Perform hierarchical clustering based on forest distances\"\"\"\n",
    "\t\tn_samples = len(X)\n",
    "\n",
    "\t\t# Calculate distance matrix\n",
    "\t\tdistance_matrix = np.zeros((n_samples, n_samples))\n",
    "\t\t\n",
    "\t\tdistance_func = self.distance_functions[self.distance_type]\n",
    "\t\tfor i in range(n_samples):\n",
    "\t\t\tfor j in range(i + 1, n_samples):\n",
    "\t\t\t\t# print(\"Xi:\", len(X[i]))\n",
    "\t\t\t\tdist = distance_func(self.forest, X[i], X[j])\n",
    "\t\t\t\tdistance_matrix[i, j] = dist\n",
    "\t\t\t\tdistance_matrix[j, i] = dist\n",
    "\t\t\t\t\n",
    "\t\t# print(distance_matrix)\n",
    "\t\t# Dopo aver calcolato la distance_matrix\n",
    "\t\tplt.imshow(distance_matrix, cmap=\"viridis\")\n",
    "\t\tplt.colorbar()\n",
    "\t\tplt.title(\"Matrice delle distanze\")\n",
    "\t\tplt.show()\n",
    "\n",
    "\t\t# Perform hierarchical clustering\n",
    "\t\tcondensed_dist = squareform(distance_matrix) # squareform trasforma la matrice quadrata in un array condensato, richiesto dalla funzione linkage.\n",
    "\t\tlinkage_matrix = linkage(condensed_dist, method='average') # linkage costruisce un albero gerarchico (dendrogramma) usando il metodo di collegamento “average”\n",
    "\t\tcluster_labels = fcluster(linkage_matrix, self.n_clusters, criterion='maxclust') # fcluster taglia l’albero in n_clusters cluster, restituendo le etichette dei cluster.\n",
    "\n",
    "\t\treturn cluster_labels - 1  # parto dal cluster 0, invece che dall'1\n",
    "\t\n",
    "\tdef purity(self, true_labels, cluster_labels):\n",
    "\t\t\"\"\"Calculate cluster purity\"\"\"\n",
    "\t\tn_samples = len(true_labels)\n",
    "\t\tpurity = 0\n",
    "\n",
    "\t\tfor cluster_id in set(cluster_labels):\n",
    "\t\t\tcluster_mask = np.array(cluster_labels) == cluster_id\n",
    "\t\t\tcluster_true_labels = np.array(true_labels)[cluster_mask]\n",
    "\n",
    "\t\t\tif len(cluster_true_labels) > 0:\n",
    "\t\t\t\tmost_common_class = Counter(cluster_true_labels).most_common(1)[0][1]\n",
    "\t\t\t\tpurity += most_common_class\n",
    "\n",
    "\t\treturn purity / n_samples\n",
    "\n",
    "\tdef entropy(self, true_labels, cluster_labels):\n",
    "\t\t\"\"\"Calculate cluster entropy\"\"\"\n",
    "\t\ttotal_entropy = 0\n",
    "\t\tn_samples = len(true_labels)\n",
    "\n",
    "\t\tfor cluster_id in set(cluster_labels):\n",
    "\t\t\tcluster_mask = np.array(cluster_labels) == cluster_id\n",
    "\t\t\tcluster_true_labels = np.array(true_labels)[cluster_mask]\n",
    "\t\t\tcluster_size = len(cluster_true_labels)\n",
    "\n",
    "\t\t\tif cluster_size > 0:\n",
    "\t\t\t\tclass_counts = Counter(cluster_true_labels)\n",
    "\t\t\t\tcluster_entropy = 0\n",
    "\n",
    "\t\t\t\tfor count in class_counts.values():\n",
    "\t\t\t\t\tif count > 0:\n",
    "\t\t\t\t\t\tprob = count / cluster_size\n",
    "\t\t\t\t\t\tcluster_entropy -= prob * np.log2(prob)\n",
    "\n",
    "\t\t\t\ttotal_entropy += (cluster_size / n_samples) * cluster_entropy\n",
    "\n",
    "\t\treturn total_entropy\n",
    "\n",
    "\tdef adjusted_rand_index(self, true_labels, cluster_labels):\n",
    "\t\t\"\"\"Calculate Adjusted Rand Index between two clusterings\n",
    "\t\tvaluta quante coppie di punti sono assegnate allo stesso cluster \n",
    "\t\tin entrambe le partizioni rispetto a quanto ci si aspetterebbe per caso\"\"\"\n",
    "\t\t\n",
    "\t\t# ARI = 1 > le due partizioni sono identiche, ARI ≈ 0 -> la somiglianza è quella che ci si aspetta per caso.\n",
    "\t\treturn adjusted_rand_score(true_labels, cluster_labels) # funzione di sklearn\n",
    "\n",
    "\tdef intra_cluster_distance(self, X, cluster_labels):\n",
    "\t\t\"\"\"Average intra-cluster distance, Euclidian distance\"\"\"\n",
    "\t\ttotal_distance = 0\n",
    "\t\ttotal_pairs = 0\n",
    "\n",
    "\t\tfor cluster_id in set(cluster_labels): # insieme di cluster unici, tipo {0, 1, 2, 3, 4}\n",
    "\t\t\tcluster_indices = [i for i, label in enumerate(cluster_labels) # per ogni cluster, tiro fuori i sample appartenenti a quel cluster\n",
    "\t\t\t\t\t\t\t if label == cluster_id]\n",
    "\t\t\t# print(\"cluster indices:\", cluster_indices)\n",
    "\t\t\tfor i in range(len(cluster_indices)):\n",
    "\t\t\t\tfor j in range(i + 1, len(cluster_indices)):\n",
    "\t\t\t\t\tidx1, idx2 = cluster_indices[i], cluster_indices[j]\n",
    "\t\t\t\t\tdist = self.euclidean_distance(X[idx1], X[idx2])\n",
    "\t\t\t\t\ttotal_distance += dist\n",
    "\t\t\t\t\ttotal_pairs += 1\n",
    "\n",
    "\t\treturn total_distance / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "\tdef inter_cluster_distance(self, X, cluster_labels):\n",
    "\t\t\"\"\"Average inter-cluster distance, Euclidian distance\"\"\"\n",
    "\t\ttotal_distance = 0\n",
    "\t\ttotal_pairs = 0\n",
    "\n",
    "\t\tunique_clusters = list(set(cluster_labels))\n",
    "\n",
    "\t\tfor i in range(len(unique_clusters)):\n",
    "\t\t\tfor j in range(i + 1, len(unique_clusters)):\n",
    "\t\t\t\tcluster1_indices = [k for k, label in enumerate(cluster_labels)\n",
    "\t\t\t\t\t\t\t\t  if label == unique_clusters[i]]\n",
    "\t\t\t\tcluster2_indices = [k for k, label in enumerate(cluster_labels)\n",
    "\t\t\t\t\t\t\t\t  if label == unique_clusters[j]]\n",
    "\n",
    "\t\t\t\tfor idx1 in cluster1_indices:\n",
    "\t\t\t\t\tfor idx2 in cluster2_indices:\n",
    "\t\t\t\t\t\tdist = self.euclidean_distance(X[idx1], X[idx2])\n",
    "\t\t\t\t\t\ttotal_distance += dist\n",
    "\t\t\t\t\t\ttotal_pairs += 1\n",
    "\n",
    "\t\treturn total_distance / total_pairs if total_pairs > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdc22c",
   "metadata": {},
   "source": [
    "### **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# 1. DATA LOADING\n",
    "# ======================\n",
    "print(\"\\n======================\")\n",
    "print(\"1. DATA LOADING\")\n",
    "print(\"======================\")\n",
    "\n",
    "# Carica dataset (scegli univariato o multivariato)\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\"C:/Users/Simone/Desktop/UNIVERSITA/MAGISTRALE/BIOMEDICAL DECISION SUPPORT SYSTEM/Univariate_ts/Car/Car_TRAIN.ts\") \n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\"C:/Users/Simone/Desktop/UNIVERSITA/MAGISTRALE/BIOMEDICAL DECISION SUPPORT SYSTEM/Univariate_ts/Car/Car_TEST.ts\")\n",
    "\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\"C:/Users/Simone/Desktop/UNIVERSITA/MAGISTRALE/BIOMEDICAL DECISION SUPPORT SYSTEM/Multivariate_ts/HandMovementDirection/HandMovementDirection_TRAIN.ts\") \n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\"C:/Users/Simone/Desktop/UNIVERSITA/MAGISTRALE/BIOMEDICAL DECISION SUPPORT SYSTEM/Multivariate_ts/HandMovementDirection/HandMovementDirection_TEST.ts\") \n",
    "\n",
    "X_train, y_train = load_from_tsfile_to_dataframe(\"C:/Users/Simone/Desktop/UNIVERSITA/MAGISTRALE/BIOMEDICAL DECISION SUPPORT SYSTEM/Univariate_ts/Beef/Beef_TRAIN.ts\") \n",
    "X_test, y_test = load_from_tsfile_to_dataframe(\"C:/Users/Simone/Desktop/UNIVERSITA/MAGISTRALE/BIOMEDICAL DECISION SUPPORT SYSTEM/Univariate_ts/Beef/Beef_TEST.ts\")\n",
    "\n",
    "# X_train, y_train = load_from_tsfile_to_dataframe(\"C:/Users/Simone/Desktop/UNIVERSITA/MAGISTRALE/BIOMEDICAL DECISION SUPPORT SYSTEM//Multivariate_ts/AtrialFibrillation/AtrialFibrillation_TRAIN.ts\") \n",
    "# X_test, y_test = load_from_tsfile_to_dataframe(\"C:/Users/Simone/Desktop/UNIVERSITA/MAGISTRALE/BIOMEDICAL DECISION SUPPORT SYSTEM/Multivariate_ts/AtrialFibrillation/AtrialFibrillation_TEST.ts\") \n",
    "\n",
    "# Split: train, validation, calibration\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "\tX_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "X_train, X_cal, y_train, y_cal = train_test_split(\n",
    "\tX_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Encoding labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val   = le.transform(y_val)\n",
    "y_cal   = le.transform(y_cal)\n",
    "y_test  = le.transform(y_test)\n",
    "\n",
    "print(\"X_train:\", len(X_train))\n",
    "print(\"y_train:\", len(y_train))\n",
    "print(\"X_val:\", len(X_val))\n",
    "print(\"y_val:\", len(y_val))\n",
    "print(\"X_cal:\", len(X_cal))\n",
    "print(\"y_cal:\", len(y_cal))\n",
    "print(\"X_test:\", len(X_test))\n",
    "print(\"y_test:\", len(y_test))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 2. DEBUG CHECK\n",
    "# ======================\n",
    "print(\"\\n======================\")\n",
    "print(\"2. DEBUG CHECK\")\n",
    "print(\"======================\")\n",
    "\n",
    "for i in range(2):\n",
    "\tprint(f\"Trace {i}:\")\n",
    "\tfor col in X_train.columns:\n",
    "\t\tserie = X_train.iloc[i][col]\n",
    "\t\tprint(f\"  Column {col}: {serie.head(10).tolist()}\")\n",
    "\tprint(\"Label:\", y_train[i])\n",
    "\tprint(\"----\")\n",
    "\n",
    "print(\"\\nShape of train set:\", X_train.shape)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3. PROMPT TREE\n",
    "# ======================\n",
    "print(\"\\n======================\")\n",
    "print(\"3. PROMPT TREE\")\n",
    "print(\"======================\")\n",
    "\n",
    "distance_function = euclidean_distance # cosine_distance, euclidean_distance, manhattan_distance\n",
    "prompt_tree = PromptTree(X_train, y_train, distance_function)\n",
    "\n",
    "# Funzione per stampare l’albero\n",
    "def print_tree(node, prefix=\"\"):\n",
    "\tif isinstance(node, LeafNode):\n",
    "\t\tprint(f\"{prefix}└── Leaf: class={node.classification_f}\")\n",
    "\telif isinstance(node, Node):\n",
    "\t\tprint(f\"{prefix}└── Node: channel={node.channel}, \"\n",
    "\t\t\t  f\"begin={node.begin_idx}, end={node.end_idx}, \"\n",
    "\t\t\t  f\"threshold={node.threshold}\")\n",
    "\t\tif node.node_sx is not None:\n",
    "\t\t\tprint(f\"{prefix}    Left:\")\n",
    "\t\t\tprint_tree(node.node_sx, prefix + \"        \")\n",
    "\t\tif node.node_dx is not None:\n",
    "\t\t\tprint(f\"{prefix}    Right:\")\n",
    "\t\t\tprint_tree(node.node_dx, prefix + \"        \")\n",
    "\n",
    "# Funzione di predizione\n",
    "def predict(tree, x, default_class=None):\n",
    "\tnode = tree.root\n",
    "\twhile isinstance(node, Node):\n",
    "\t\tslice_x = x[node.channel][node.begin_idx:node.end_idx]\n",
    "\t\tdist = np.mean(node.distance_f_node(slice_x, node.x_barr))\n",
    "\t\tnode = node.node_sx if dist <= node.threshold else node.node_dx\n",
    "\n",
    "\tif not node.classification_f:\n",
    "\t\treturn default_class\n",
    "\treturn max(node.classification_f.items(), key=lambda kv: kv[1])[0]\n",
    "\n",
    "# Accuracy su test con PromptTree singolo\n",
    "y_pred = [predict(prompt_tree, x, default_class=0) for _, x in X_test.iterrows()]\n",
    "acc = round(np.mean([yp == yt for yp, yt in zip(y_pred, y_test)]), 3)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 4. RANDOM FOREST\n",
    "# ======================\n",
    "print(\"\\n======================\")\n",
    "print(\"4. RANDOM FOREST\")\n",
    "print(\"======================\")\n",
    "\n",
    "n_trees = 5\n",
    "distance_function = cosine_distance\n",
    "random_forest = RandomForest(n_trees, X_train, y_train, X_val, y_val, X_test, y_test, distance_function)\n",
    "\n",
    "return_probs = True\n",
    "\n",
    "# Confusion matrix helper\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "\tcm = confusion_matrix(y_true, y_pred)\n",
    "\tplt.figure(figsize=(6,5))\n",
    "\tsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\tplt.xlabel(\"Predicted\")\n",
    "\tplt.ylabel(\"True\")\n",
    "\tplt.title(title)\n",
    "\tplt.show()\n",
    "\n",
    "# Majority voting\n",
    "print(\"\\n--- Majority Voting ---\")\n",
    "acc_maj, y_pred_maj, y_probs_maj = random_forest.majority_voting(random_forest.X_test, return_probs=return_probs)\n",
    "print(\"Accuracy:\", acc_maj)\n",
    "print(\"Classification Report:\\n\", classification_report(random_forest.y_test, y_pred_maj))\n",
    "plot_confusion_matrix(random_forest.y_test, y_pred_maj, \"Majority Voting\")\n",
    "\n",
    "# Weighted voting\n",
    "print(\"\\n--- Weighted Voting ---\")\n",
    "acc_wei, y_pred_wei, y_probs_wei = random_forest.weighted_voting(\n",
    "\trandom_forest.X_val, random_forest.y_val,\n",
    "\trandom_forest.X_test, random_forest.y_test,\n",
    "\treturn_probs=return_probs\n",
    ")\n",
    "print(\"Accuracy:\", acc_wei)\n",
    "print(\"Classification Report:\\n\", classification_report(random_forest.y_test, y_pred_wei))\n",
    "plot_confusion_matrix(random_forest.y_test, y_pred_wei, \"Weighted Voting\")\n",
    "\n",
    "# Track record voting\n",
    "print(\"\\n--- Track Record Voting ---\")\n",
    "acc_tr, y_pred_tr, y_probs_tr = random_forest.track_record_voting(\n",
    "\trandom_forest.X_val, random_forest.y_val,\n",
    "\trandom_forest.X_test, random_forest.y_test,\n",
    "\treturn_probs=return_probs\n",
    ")\n",
    "print(\"Accuracy:\", acc_tr)\n",
    "print(\"Classification Report:\\n\", classification_report(random_forest.y_test, y_pred_tr))\n",
    "plot_confusion_matrix(random_forest.y_test, y_pred_tr, \"Track Record Voting\")\n",
    "\n",
    "print(\"\\nRF weights for each tree:\", random_forest.weights_random_forest)\n",
    "print(\"RF track records for each tree:\", random_forest.track_records_random_forest)\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 5. CONFORMAL CLASSIFIER\n",
    "# ======================\n",
    "print(\"\\n======================\")\n",
    "print(\"5. CONFORMAL CLASSIFIER\")\n",
    "print(\"======================\")\n",
    "\n",
    "voting = 'weighted'\n",
    "alpha = 0.1\n",
    "\n",
    "conformal_classifier = ConformalClassifier(random_forest, voting, alpha)\n",
    "confidence, efficiency, miscalibration = conformal_classifier.calibrate()\n",
    "print(f\"Confidence: {confidence*100:.1f}%\")\n",
    "print(f\"Efficiency: {efficiency*100:.1f}%\")\n",
    "print(f\"Miscalibration: {miscalibration*100:.1f}%\")\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 6. ISOLATION FOREST (CLUSTERING)\n",
    "# ======================\n",
    "print(\"\\n======================\")\n",
    "print(\"6. ISOLATION FOREST (CLUSTERING)\")\n",
    "print(\"======================\")\n",
    "\n",
    "n_clusters = 5\n",
    "distance_type = 'ratio_rf'\n",
    "isolation_forest = IsolationForest(random_forest, n_clusters, distance_type)\n",
    "\n",
    "# Flatten delle serie (concatenazione per riga)\n",
    "X_train_flat = np.array([np.concatenate([chan for chan in row]) for _, row in X_train.iterrows()])\n",
    "\n",
    "# Clustering\n",
    "cluster_labels = isolation_forest.perform_cluster(X_train_flat)\n",
    "print(\"Cluster labels:\", cluster_labels)\n",
    "print(\"Num labels == num samples:\", len(cluster_labels) == len(X_train_flat))\n",
    "\n",
    "# Metriche clustering\n",
    "intra_cluster_dist = isolation_forest.intra_cluster_distance(X_train_flat, cluster_labels)\n",
    "inter_cluster_dist = isolation_forest.inter_cluster_distance(X_train_flat, cluster_labels)\n",
    "purity = isolation_forest.purity(y_train, cluster_labels)\n",
    "entropy = isolation_forest.entropy(y_train, cluster_labels)\n",
    "ari = isolation_forest.adjusted_rand_index(y_train, cluster_labels)\n",
    "\n",
    "print(\"Intra cluster distance:\", intra_cluster_dist)\n",
    "print(\"Inter cluster distance:\", inter_cluster_dist)\n",
    "print(\"Purity:\", purity)\n",
    "print(\"Entropy:\", entropy)\n",
    "print(\"ARI:\", ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with all distances and more clusters\n",
    "\n",
    "n_clusters = 10\n",
    "distance_type = 'breiman'\n",
    "isolation_forest = IsolationForest(random_forest, n_clusters, distance_type)\n",
    "\n",
    "# Flatten delle serie (concatenazione per riga)\n",
    "X_train_flat = np.array([np.concatenate([chan for chan in row]) for _, row in X_train.iterrows()])\n",
    "\n",
    "distance_types = ['breiman', 'zhu', 'ratio_rf']\n",
    "results = []\n",
    "\n",
    "for dist in distance_types:\n",
    "\tprint(f\"\\n--- Clustering con distanza: {dist} ---\")\n",
    "\tisolation_forest = IsolationForest(random_forest, n_clusters, distance_type=dist)\n",
    "\tcluster_labels = isolation_forest.perform_cluster(X_train_flat)\n",
    "\n",
    "\tintra_cluster_dist = isolation_forest.intra_cluster_distance(X_train_flat, cluster_labels)\n",
    "\tinter_cluster_dist = isolation_forest.inter_cluster_distance(X_train_flat, cluster_labels)\n",
    "\tpurity = isolation_forest.purity(y_train, cluster_labels)\n",
    "\tentropy = isolation_forest.entropy(y_train, cluster_labels)\n",
    "\tari = isolation_forest.adjusted_rand_index(y_train, cluster_labels)\n",
    "\n",
    "\tresults.append({\n",
    "\t\t\"Distance\": dist,\n",
    "\t\t\"IntraDist\": intra_cluster_dist,\n",
    "\t\t\"InterDist\": inter_cluster_dist,\n",
    "\t\t\"Purity\": purity,\n",
    "\t\t\"Entropy\": entropy,\n",
    "\t\t\"ARI\": ari\n",
    "\t})\n",
    "\n",
    "# Confronto tabellare\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nConfronto clustering:\")\n",
    "print(df_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
